\documentclass[journal]{vgtc}                % final (journal style)
%\documentclass[review,journal]{vgtc}         % review (journal style)
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint,journal]{vgtc}       % preprint (journal style)

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Please use the ``preprint''  option when producing a preprint version
%% for sharing your article on an open access repository

%% Please note that the use of figures other than the optional teaser is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures other than the optional teaser on the
%% first page will be refused. Also, the teaser figure should only have the
%% width of the abstract as the template enforces it.

%% These few lines make a distinction between latex and pdflatex calls and they
%% bring in essential packages for graphics and font handling.
%% Note that due to the \DeclareGraphicsExtensions{} call it is no longer necessary
%% to provide the the path and extension of a graphics file:
%% \includegraphics{diamondrule} is completely sufficient.
%%
\ifpdf%                                % if we use pdflatex
  \pdfoutput=1\relax                   % create PDFs from pdfLaTeX
  \pdfcompresslevel=9                  % PDF Compression
  \pdfoptionpdfminorversion=7          % create PDF 1.7
  \ExecuteOptions{pdftex}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg} % for pdflatex we expect .pdf, .png, or .jpg files
\else%                                 % else we use pure latex
  \ExecuteOptions{dvips}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.eps}     % for pure latex we expect eps files
\fi%

%% it is recomended to use ``\autoref{sec:bla}'' instead of ``Fig.~\ref{sec:bla}''
\graphicspath{{figures/}{pictures/}{images/}{./}} % where to search for the images

\usepackage{microtype}                 % use micro-typography (slightly more compact, better to read)
\PassOptionsToPackage{warn}{textcomp}  % to address font issues with \textrightarrow
\usepackage{textcomp}                  % use better special symbols
\usepackage{mathptmx}                  % use matching math font
\usepackage{times}                     % we use Times as the main font
\renewcommand*\ttdefault{txtt}         % a nicer typewriter font
\usepackage{cite}                      % needed to automatically sort the references
\usepackage{tabu}                      % only used for the table example
\usepackage{booktabs}                  % only used for the table example
\usepackage{verbatim}
%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% In preprint mode you may define your own headline. If not, the default IEEE copyright message will appear in preprint mode.
%\preprinttext{To appear in IEEE Transactions on Visualization and Computer Graphics.}

%% In preprint mode, this adds a link to the version of the paper on IEEEXplore
%% Uncomment this line when you produce a preprint version of the article 
%% after the article receives a DOI for the paper from IEEE
%\ieeedoi{xx.xxxx/TVCG.201x.xxxxxxx}

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}
%% please declare the paper type of your paper to help reviewers, only shown in review mode
%% choices:
%% * algorithm/technique
%% * application/design study
%% * evaluation
%% * system
%% * theory/model
\vgtcpapertype{please specify}

%% Paper title.
\title{Isosurface Rendering via Deep Geometric Properties Synthesis}

%% This is how authors are specified in the journal style

%% indicate IEEE Member or Student Member in form indicated below
\author{Neng Shi, Jingyi Shen, and Haoyu Li}
\authorfooter{
%% insert punctuation at end of each item
\item
 Neng Shi, Jingyi Shen, and Haoyu Li are with the Department of Computer Science and Engineering, The Ohio State University. E-mail: {shi.1337, shen.1250, li.8460}@osu.edu.
}

%other entries to be set up for journal
\shortauthortitle{Biv \MakeLowercase{\textit{et al.}}: Global Illumination for Fun and Profit}
%\shortauthortitle{Firstauthor \MakeLowercase{\textit{et al.}}: Paper Title}

%% Abstract section.
\abstract{
} % end of abstract

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword
\keywords{Radiosity, global illumination, constant time}

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
%% The ``\CCScat'' command takes four arguments.

\CCScatlist{ % not used in journal version
 \CCScat{K.6.1}{Management of Computing and Information Systems}%
{Project and People Management}{Life Cycle};
 \CCScat{K.7.m}{The Computing Profession}{Miscellaneous}{Ethics}
}

%% Uncomment below to include a teaser figure.

%% Uncomment below to disable the manuscript note
\renewcommand{\manuscriptnotetxt}{}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace


\vgtcinsertpkg

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}

\maketitle

%% \section{Introduction} %for journal use above \firstsection{..} instead
Ensemble simulation \cite{wang2018visualization} is important in scientific visualization. For example, in meteorology, meteorologists often use numerical weather forecast models to generate aggregated simulation data. By disturbing the initial conditions or use different prediction model formulas, the collective simulation data usually consists of a series of data members. For Ensemble data \cite{ahrens2014situ, ahrens2014image}, its large number of data members and a high degree of complexity lead to two problems: (1) a long time for scientists to run the simulation and see the results and (2) I/O bottleneck for storing the raw data. Both problems can influence scientists to effectively analyze and evaluate data. In-situ visualization \cite{bauer2016situ, ma2009situ}, in which the calculation results are not stored but directly processed into images at the computing nodes, can greatly reduce the amount of data stored, transmitted, and post-processed. However, when the raw data is not available, the flexibility of post-hoc analysis would be constrained.  

Recently, with the development of deep generative models, people can generate high-quality images given random noise \cite{goodfellow2014generative} or some semantic prior information \cite{mirza2014conditional}. Specifically, in ensemble visualization, He et al. \cite{he2019insitunet} proposed InSituNet, which is a deep learning-based surrogate model to support parameter space exploration for ensemble simulations that are visualized in situ. InSituNet can generate images of high accuracy, and fidelity. However, it has a limitation that it only allows users to switch among several predefined visual mappings (e.g., transfer functions). The main reason is covering such a joint space of all possible simulation and visualization parameters requires a large number of training images.

This study tries to propose a solution for the limitation, focusing on isosurface, which needs only one scalar value (isovalue) to encode. Our final goal is to train a deep network that can generate geometric properties (e.g., depth, normal maps) of an isosurface given simulation, isosurface, and view parameters. After getting the geometric properties of an isosurface, by traditional computer graphics techniques, for example, deferred shading \cite{deering1988triangle} and depth peeling \cite{zolt2003depthpeel}, we can get isosurface or volume rendering results with different shading effects. Currently, due to the time limitation, we may only focus on isosurface and view parameters, without considering the simulation parameter.  

\section{Related Work}

In this section, we review related work in deep generative models for scientific visualization.

\subsection{Deep Generative Models for Scientific Visualization}

Deep generative models (e.g., PixelRNN \cite{oord2016pixel}, PixelCNN \cite{van2016conditional}, VAE \cite{kingma2013auto}, GAN \cite{goodfellow2014generative}, etc.) have evolved in recent years and have achieved exciting accomplishment. Given the training data, generative models can generate new samples from the same distribution. Among them, Generative adversarial networks \cite{goodfellow2014generative} (GANs) represents the highest achievement, and has been used in various applications, including image generation \cite{karras2017progressive, jin2017towards}, image-to-image translation \cite{isola2017image, zhu2017unpaired}, text-to-image translation \cite{zhang2017stackgan, reed2016generative}, super resolution \cite{ledig2017photo}, and 3D object generation \cite{wu2016learning, gadelha20173d}. 

Scientific visualization researchers increasingly use deep generative models to synthesize the visualization results. Berger et al. \cite{berger2018generative} train a generative model to synthesize volume rendering images, conditioned on viewpoints and transfer functions. He et al. \cite{he2019insitunet} proposed InSituNet, a deep learning-based surrogate model to support parameter space exploration for ensemble simulations that are visualized in situ. Weiss et al. \cite{weiss2019volumetric} train a super-resolution network that can upscale a low-resolution sampling of isosurface depths and normals to a high resolution one. Hong et al. \cite{hong2019dnn} implement a deep neural network that accepts original images with goal rendering effect and new viewing parameters as the inputs and synthesizes new images. Some other researchers focus on directly predict 3D data. To address a spatial super-resolution problem, Xie et al. \cite{xie2018tempogan} propose a temporally coherent generative model to generate high-resolution fluid flows. Han et al. \cite{han2019tsr} achieve temporal super-resolution of time-varying data by a recurrent generative network (RGN, a combination of RNN and GAN).

Our work is closely related to InSituNet \cite{he2019insitunet}, which focus on parameter space exploration. The difference between our work and InSituNet is that we focus on synthesize the  geometric properties (e.g., depth,normal maps) of an isosurface, rather than visualization images. This idea is similar with the super-resolution work by Weiss et al. \cite{weiss2019volumetric}. 


\section{Isosurface Learning}

Our method will consist of three major parts. The first part is about the training data generation. Since we do not consider simulation parameters, we will use volumes available online in large volume collections. Then, given an isovalue and viewpoint, we can use OpenGL \cite{woo1999opengl} to implement volumetric ray-casting to get corresponding geometric properties of an isosurface. Second, with the collected pairs between parameters and the corresponding geometric properties, we will train a single or a set of networks to learn the mapping from input parameters to geometric properties. We will consider different state-of-the-art learning techniques for deep generative models, including the model structure and loss function design, etc. Third, we will do screen-space shading based on the synthesized geometric properties. We can control the lighting conditions and get some interesting visual effects. 

\subsection{Input Data}
Given an isovalue, we implement volumetric ray-casting to get corresponding geometric properties of an isosurface. We let the ray advance through the volume until reaching a sample whose intensity is greater than or equal to the threshold value, meaning that we hit the isosurface. Once we find a point belonging to the isosurface, instead of directly applying an illumination model to perform shading, we use the geometry pass in deferred shading \cite{deering1988triangle}. We store all kinds of geometric properties in a collection of textures called the G-buffer. In our study, these geometric properties are maps of size $ H \times W $,  including:

\begin{itemize}
\item $M \in [0, 1] ^ {H \times W}$: The mask binary map in which 1 is for foreground and 0 otherwise. After network prediction, each pixel is considered foreground if its predicted probability in the mask map is above 50\%.  

\item $N \in [-1, +1] ^ {3 \times H \times W}$: The normal map with the normal vectors in view space.

\item $P \in [0, 1] ^ {3 \times H \times W}$: The position map with the position vectors in local space. We choose local space position because it already normalized within the range $[0, 1]$.

\item $D \in [-1, +1] ^ {H \times W}$: The depth map. 

\end{itemize}

We will consider adding more features such as ambient occlusion to make our study more powerful.  

\subsection{Network Training}

\subsubsection{Network architecture}
We will build the generator following InSituNet proposed by He et al. \cite{he2019insitunet} since our geometric property synthesis problem is similar to their image synthesis problem. We can train a set of networks, each of which is used to learn the mapping from input parameters to one geometric property. Or we can concatenate all the geometric properties and use one single network to learn the mapping. The only difference between these two options is the number of convolution kernels in the last layer. 

\subsubsection{Loss Function}
We will attempt to estimate the network parameters to minimize a loss function. Our loss function may consist of following terms: (a) differences between ground-truth and predicted geometric properties (normal, position and depth map),  (b) disagreement between ground-truth and predicted foreground masks, (c) large-scale structural difference between the predicted and training maps.

\paragraph{Normal, position and depth loss}
We will first consider the difference between predicted geometric properties and the ground-truth. There is 3 options for the loss: (a) Spatial loss. We will use $l1$ distance for each geometric property, which will only be calculated for pixels marked as foreground in the ground-truth. (b) Perceptual Loss based on VGGNet \cite{simonyan2014very}. (c) Adversarial loss. It is trying to measure the difference between two image distributions. We will train a discriminator along with the previously mentioned generator to differentiate the generated geometric properties with the ground truth. Our discriminator will also be designed following InSituNet \cite{he2019insitunet}. 

\paragraph{Mask loss}
We may use $l1$ distance or cross-entropy function commonly used in classification to penalize the difference between predicted and ground-truth foreground labeling.

\paragraph{Large-scale structural difference}
We may also penalize overall structural information with the help of adversarial loss. While training a single network, instead of adding adversarial loss term for each map, we will concatenate all the map channels and feed them into the discriminator. We think, to some extent, it can help to make different predicted maps fit each other.   

\subsection{screen-space shading}
We can use render a screen-filled quad and calculate the scene's lighting for each fragment using the geometrical information predicted by the generator. In this way, we can change the lighting conditions to better visualize the object we are interested in. We can also implement some advanced lighting effects. For example, by comparing the depth map predicted from the lighting viewpoint and camera viewpoint, we can perform shadow mapping \cite{williams1978casting}. Another plan for us is to apply volumetric depth-peeling \cite{zolt2003depthpeel},  rendering interior and exterior isosurfaces and blending them to get volume rendering results. Then we can use different transfer functions to get different visual effects.  


\section{Project milestone}

Here, we decompose our projects into several tasks and provide a draft for how these tasks are scheduled before the deadline.

\begin{itemize}
\item Ray casting based training data generation: Done
\item Train networks and tune the hyper-parameters based on isosurface rendering results: In progress, by 4/7.
\item Finish shadow mapping and Depth-Peeling for volume rendering: by 4/7
\item write final paper and prepare presentation: by 4/21. 

\end{itemize}

%\bibliographystyle{abbrv}
\bibliographystyle{abbrv-doi}
%\bibliographystyle{abbrv-doi-narrow}
%\bibliographystyle{abbrv-doi-hyperref}
%\bibliographystyle{abbrv-doi-hyperref-narrow}

\bibliography{template}
\end{document}

